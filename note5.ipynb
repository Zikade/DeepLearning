{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a184ab6780>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(Image.fromarray((mnist.train.images[1] * 255).astype(np.uint8).reshape([28, 28])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(Image.fromarray(mnist.train.images[0].reshape([28, 28])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用小批量\n",
    "batch_images, batch_labels = mnist.train.next_batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 784)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    inputs = tf.placeholder(\n",
    "        shape=[None, 784], dtype=tf.float32)\n",
    "    labels = tf.placeholder(\n",
    "        shape=[None, 10], dtype=tf.float32)\n",
    "    \n",
    "    out_weight = tf.Variable(\n",
    "        tf.random_normal([784, 10]))\n",
    "    out_bias = tf.Variable(tf.zeros([10, ]))\n",
    "    \n",
    "    logits = tf.matmul(inputs, out_weight) + out_bias\n",
    "    \n",
    "    output = tf.nn.softmax(logits) # [none, 10]\n",
    "    \n",
    "    loss = tf.reduce_mean(\n",
    "        -tf.reduce_sum(\n",
    "            labels * tf.log(output + 1e-17), axis=1))\n",
    "    \n",
    "    acc = tf.reduce_mean(tf.cast(\n",
    "        tf.equal(tf.argmax(output, axis=1),\n",
    "                 tf.argmax(labels, axis=1)), tf.float32))\n",
    "    \n",
    "    optim = tf.train.GradientDescentOptimizer(\n",
    "        learning_rate=0.01)\n",
    "    train_op = optim.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     0, loss 15.9256, acc 0.0802\n",
      "step   500, loss 5.3623, acc 0.2371\n",
      "step  1000, loss 4.5779, acc 0.4014\n",
      "step  1500, loss 2.7865, acc 0.5046\n",
      "step  2000, loss 2.2100, acc 0.5728\n",
      "step  2500, loss 1.9900, acc 0.6250\n",
      "step  3000, loss 2.0538, acc 0.6600\n",
      "step  3500, loss 1.2781, acc 0.6965\n",
      "step  4000, loss 1.1578, acc 0.7069\n",
      "step  4500, loss 1.2189, acc 0.7243\n",
      "step  5000, loss 1.4226, acc 0.7413\n",
      "step  5500, loss 1.3907, acc 0.7510\n",
      "step  6000, loss 2.2092, acc 0.7650\n",
      "step  6500, loss 0.8933, acc 0.7685\n",
      "step  7000, loss 2.3060, acc 0.7754\n",
      "step  7500, loss 0.9287, acc 0.7887\n",
      "step  8000, loss 1.7200, acc 0.7873\n",
      "step  8500, loss 0.6559, acc 0.7942\n",
      "step  9000, loss 0.6806, acc 0.7972\n",
      "step  9500, loss 1.4256, acc 0.8069\n",
      "step 10000, loss 0.9596, acc 0.8049\n",
      "step 10500, loss 0.6836, acc 0.8120\n",
      "step 11000, loss 0.8711, acc 0.8119\n",
      "step 11500, loss 0.7327, acc 0.8164\n",
      "step 12000, loss 1.2368, acc 0.8164\n",
      "step 12500, loss 1.6132, acc 0.8194\n",
      "step 13000, loss 0.7434, acc 0.8209\n",
      "step 13500, loss 0.7248, acc 0.8297\n",
      "step 14000, loss 0.8137, acc 0.8264\n",
      "step 14500, loss 1.2485, acc 0.8348\n",
      "step 15000, loss 0.5513, acc 0.8257\n",
      "step 15500, loss 0.6559, acc 0.8350\n",
      "step 16000, loss 1.0141, acc 0.8339\n",
      "step 16500, loss 1.2963, acc 0.8364\n",
      "step 17000, loss 0.6481, acc 0.8367\n",
      "step 17500, loss 0.7189, acc 0.8397\n",
      "step 18000, loss 0.5054, acc 0.8362\n",
      "step 18500, loss 0.6632, acc 0.8416\n",
      "step 19000, loss 0.2852, acc 0.8425\n",
      "step 19500, loss 1.0854, acc 0.8451\n",
      "step 20000, loss 0.3732, acc 0.8433\n",
      "step 20500, loss 0.4251, acc 0.8473\n",
      "step 21000, loss 0.7304, acc 0.8472\n",
      "step 21500, loss 0.7995, acc 0.8478\n",
      "step 22000, loss 1.5805, acc 0.8521\n",
      "step 22500, loss 0.6424, acc 0.8490\n",
      "step 23000, loss 0.0777, acc 0.8515\n",
      "step 23500, loss 0.5241, acc 0.8536\n",
      "step 24000, loss 1.1155, acc 0.8492\n",
      "step 24500, loss 0.6646, acc 0.8583\n",
      "step 25000, loss 0.9213, acc 0.8527\n",
      "step 25500, loss 0.4359, acc 0.8559\n",
      "step 26000, loss 0.9451, acc 0.8617\n",
      "step 26500, loss 0.5353, acc 0.8557\n",
      "step 27000, loss 0.6487, acc 0.8570\n",
      "step 27500, loss 0.6216, acc 0.8602\n",
      "step 28000, loss 0.8486, acc 0.8609\n",
      "step 28500, loss 0.7096, acc 0.8619\n",
      "step 29000, loss 0.7098, acc 0.8596\n",
      "step 29500, loss 0.3712, acc 0.8621\n",
      "step 30000, loss 1.0320, acc 0.8626\n",
      "step 30500, loss 1.1462, acc 0.8655\n",
      "step 31000, loss 0.9862, acc 0.8612\n",
      "step 31500, loss 1.2949, acc 0.8648\n",
      "step 32000, loss 0.9976, acc 0.8646\n",
      "step 32500, loss 0.3085, acc 0.8679\n",
      "step 33000, loss 0.6091, acc 0.8646\n",
      "step 33500, loss 0.4499, acc 0.8689\n",
      "step 34000, loss 0.1653, acc 0.8652\n",
      "step 34500, loss 0.5502, acc 0.8695\n",
      "step 35000, loss 0.2196, acc 0.8689\n",
      "step 35500, loss 1.0106, acc 0.8706\n",
      "step 36000, loss 0.8241, acc 0.8706\n",
      "step 36500, loss 1.2340, acc 0.8707\n",
      "step 37000, loss 0.2731, acc 0.8721\n",
      "step 37500, loss 0.8915, acc 0.8687\n",
      "step 38000, loss 0.2363, acc 0.8722\n",
      "step 38500, loss 0.3794, acc 0.8734\n",
      "step 39000, loss 0.3169, acc 0.8708\n",
      "step 39500, loss 0.2122, acc 0.8751\n",
      "step 40000, loss 0.8099, acc 0.8742\n",
      "step 40500, loss 1.4419, acc 0.8741\n",
      "step 41000, loss 1.0773, acc 0.8727\n",
      "step 41500, loss 0.3405, acc 0.8779\n",
      "step 42000, loss 0.7334, acc 0.8743\n",
      "step 42500, loss 0.2181, acc 0.8741\n",
      "step 43000, loss 0.2326, acc 0.8752\n",
      "step 43500, loss 0.8764, acc 0.8748\n",
      "step 44000, loss 0.5560, acc 0.8779\n",
      "step 44500, loss 0.3496, acc 0.8814\n",
      "step 45000, loss 0.1846, acc 0.8740\n",
      "step 45500, loss 0.2983, acc 0.8758\n",
      "step 46000, loss 0.3575, acc 0.8773\n",
      "step 46500, loss 0.8266, acc 0.8796\n",
      "step 47000, loss 0.8976, acc 0.8777\n",
      "step 47500, loss 0.1227, acc 0.8779\n",
      "step 48000, loss 0.2969, acc 0.8798\n",
      "step 48500, loss 0.9845, acc 0.8795\n",
      "step 49000, loss 0.7773, acc 0.8758\n",
      "step 49500, loss 0.4489, acc 0.8786\n",
      "step 50000, loss 0.6355, acc 0.8827\n",
      "step 50500, loss 0.0539, acc 0.8743\n",
      "step 51000, loss 0.1792, acc 0.8847\n",
      "step 51500, loss 0.1093, acc 0.8772\n",
      "step 52000, loss 0.1850, acc 0.8792\n",
      "step 52500, loss 0.1461, acc 0.8807\n",
      "step 53000, loss 0.3587, acc 0.8799\n",
      "step 53500, loss 0.6492, acc 0.8814\n",
      "step 54000, loss 0.4941, acc 0.8865\n",
      "step 54500, loss 0.2709, acc 0.8772\n",
      "step 55000, loss 0.2976, acc 0.8836\n",
      "step 55500, loss 0.1699, acc 0.8786\n",
      "step 56000, loss 0.1373, acc 0.8867\n",
      "step 56500, loss 0.8321, acc 0.8797\n",
      "step 57000, loss 0.5310, acc 0.8800\n",
      "step 57500, loss 0.2208, acc 0.8848\n",
      "step 58000, loss 0.2412, acc 0.8829\n",
      "step 58500, loss 0.4479, acc 0.8823\n",
      "step 59000, loss 1.1517, acc 0.8832\n",
      "step 59500, loss 0.4380, acc 0.8841\n",
      "step 60000, loss 0.2765, acc 0.8849\n",
      "step 60500, loss 0.3135, acc 0.8850\n",
      "step 61000, loss 0.8827, acc 0.8869\n",
      "step 61500, loss 0.2699, acc 0.8818\n",
      "step 62000, loss 0.3246, acc 0.8911\n",
      "step 62500, loss 0.1617, acc 0.8818\n",
      "step 63000, loss 0.4866, acc 0.8821\n",
      "step 63500, loss 1.0925, acc 0.8859\n",
      "step 64000, loss 0.5328, acc 0.8852\n",
      "step 64500, loss 0.5321, acc 0.8857\n",
      "step 65000, loss 0.3178, acc 0.8869\n",
      "step 65500, loss 0.5661, acc 0.8884\n",
      "step 66000, loss 0.1902, acc 0.8873\n",
      "step 66500, loss 0.1572, acc 0.8841\n",
      "step 67000, loss 0.1400, acc 0.8911\n",
      "step 67500, loss 0.3808, acc 0.8849\n",
      "step 68000, loss 0.1082, acc 0.8840\n",
      "step 68500, loss 0.3690, acc 0.8863\n",
      "step 69000, loss 0.3096, acc 0.8897\n",
      "step 69500, loss 1.3566, acc 0.8856\n",
      "step 70000, loss 0.1477, acc 0.8889\n",
      "step 70500, loss 0.3994, acc 0.8885\n",
      "step 71000, loss 0.2405, acc 0.8882\n",
      "step 71500, loss 1.3207, acc 0.8863\n",
      "step 72000, loss 0.3680, acc 0.8915\n",
      "step 72500, loss 0.3591, acc 0.8883\n",
      "step 73000, loss 0.6122, acc 0.8894\n",
      "step 73500, loss 0.0899, acc 0.8829\n",
      "step 74000, loss 0.3197, acc 0.8922\n",
      "step 74500, loss 0.5724, acc 0.8862\n",
      "step 75000, loss 0.1319, acc 0.8878\n",
      "step 75500, loss 0.9196, acc 0.8915\n",
      "step 76000, loss 0.8086, acc 0.8850\n",
      "step 76500, loss 0.1570, acc 0.8884\n",
      "step 77000, loss 0.6381, acc 0.8928\n",
      "step 77500, loss 0.9014, acc 0.8882\n",
      "step 78000, loss 0.1518, acc 0.8933\n",
      "step 78500, loss 0.4870, acc 0.8910\n",
      "step 79000, loss 0.0846, acc 0.8877\n",
      "step 79500, loss 0.7595, acc 0.8883\n",
      "step 80000, loss 0.9410, acc 0.8908\n",
      "step 80500, loss 0.5310, acc 0.8909\n",
      "step 81000, loss 0.3433, acc 0.8939\n",
      "step 81500, loss 0.1724, acc 0.8874\n",
      "step 82000, loss 0.6470, acc 0.8911\n",
      "step 82500, loss 1.4820, acc 0.8901\n",
      "step 83000, loss 0.7101, acc 0.8931\n",
      "step 83500, loss 0.2233, acc 0.8901\n",
      "step 84000, loss 0.5471, acc 0.8888\n",
      "step 84500, loss 0.5215, acc 0.8900\n",
      "step 85000, loss 0.4014, acc 0.8918\n",
      "step 85500, loss 0.6844, acc 0.8924\n",
      "step 86000, loss 0.1122, acc 0.8928\n",
      "step 86500, loss 0.2652, acc 0.8910\n",
      "step 87000, loss 0.6608, acc 0.8894\n",
      "step 87500, loss 0.4714, acc 0.8929\n",
      "step 88000, loss 0.4073, acc 0.8935\n",
      "step 88500, loss 0.3117, acc 0.8942\n",
      "step 89000, loss 0.8175, acc 0.8914\n",
      "step 89500, loss 0.5440, acc 0.8924\n",
      "step 90000, loss 0.1751, acc 0.8944\n",
      "step 90500, loss 0.6551, acc 0.8879\n",
      "step 91000, loss 0.1047, acc 0.8946\n",
      "step 91500, loss 0.7169, acc 0.8931\n",
      "step 92000, loss 0.5632, acc 0.8944\n",
      "step 92500, loss 0.9032, acc 0.8899\n",
      "step 93000, loss 0.6091, acc 0.8958\n",
      "step 93500, loss 0.6616, acc 0.8904\n",
      "step 94000, loss 0.4555, acc 0.8908\n",
      "step 94500, loss 0.0821, acc 0.8955\n",
      "step 95000, loss 0.2818, acc 0.8947\n",
      "step 95500, loss 0.4139, acc 0.8930\n",
      "step 96000, loss 0.6801, acc 0.8892\n",
      "step 96500, loss 1.0993, acc 0.8989\n",
      "step 97000, loss 0.1727, acc 0.8917\n",
      "step 97500, loss 0.3391, acc 0.8988\n",
      "step 98000, loss 0.4378, acc 0.8909\n",
      "step 98500, loss 0.3159, acc 0.8972\n",
      "step 99000, loss 0.4059, acc 0.8922\n",
      "step 99500, loss 0.1063, acc 0.8955\n",
      "step 100000, loss 0.1701, acc 0.8941\n",
      "step 100500, loss 0.5261, acc 0.8960\n",
      "step 101000, loss 0.7716, acc 0.8952\n",
      "step 101500, loss 0.3948, acc 0.8947\n",
      "step 102000, loss 1.0416, acc 0.8955\n",
      "step 102500, loss 0.2970, acc 0.8977\n",
      "step 103000, loss 0.4605, acc 0.8937\n",
      "step 103500, loss 0.3676, acc 0.8962\n",
      "step 104000, loss 0.2787, acc 0.8961\n",
      "step 104500, loss 0.0790, acc 0.8991\n",
      "step 105000, loss 0.5798, acc 0.8993\n",
      "step 105500, loss 0.4436, acc 0.8927\n",
      "step 106000, loss 0.6634, acc 0.8971\n",
      "step 106500, loss 0.2984, acc 0.8977\n",
      "step 107000, loss 0.4271, acc 0.8953\n",
      "step 107500, loss 0.4041, acc 0.8987\n",
      "step 108000, loss 0.6659, acc 0.8918\n",
      "step 108500, loss 0.4365, acc 0.8965\n",
      "step 109000, loss 0.2840, acc 0.9001\n",
      "step 109500, loss 0.4589, acc 0.8951\n",
      "step 110000, loss 0.1189, acc 0.8983\n",
      "step 110500, loss 0.1158, acc 0.8949\n",
      "step 111000, loss 0.5980, acc 0.8984\n",
      "step 111500, loss 0.6115, acc 0.8971\n",
      "step 112000, loss 0.1634, acc 0.8963\n",
      "step 112500, loss 0.3635, acc 0.8957\n",
      "step 113000, loss 0.2697, acc 0.8980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 113500, loss 0.1319, acc 0.8975\n",
      "step 114000, loss 0.6900, acc 0.8995\n",
      "step 114500, loss 0.3463, acc 0.8975\n",
      "step 115000, loss 0.5804, acc 0.8978\n",
      "step 115500, loss 0.7298, acc 0.8958\n",
      "step 116000, loss 0.3478, acc 0.8988\n",
      "step 116500, loss 0.4462, acc 0.9005\n",
      "step 117000, loss 0.4412, acc 0.8940\n",
      "step 117500, loss 0.6107, acc 0.8992\n",
      "step 118000, loss 0.0693, acc 0.8945\n",
      "step 118500, loss 0.0563, acc 0.9036\n",
      "step 119000, loss 0.3536, acc 0.8935\n",
      "step 119500, loss 0.3768, acc 0.9012\n",
      "step 120000, loss 0.0849, acc 0.8963\n",
      "step 120500, loss 0.2032, acc 0.9030\n",
      "step 121000, loss 0.4888, acc 0.8930\n",
      "step 121500, loss 0.2378, acc 0.8985\n",
      "step 122000, loss 0.2202, acc 0.9004\n",
      "step 122500, loss 0.4702, acc 0.8958\n",
      "step 123000, loss 0.1154, acc 0.9011\n",
      "step 123500, loss 0.5730, acc 0.9014\n",
      "step 124000, loss 0.2403, acc 0.8962\n",
      "step 124500, loss 0.4531, acc 0.9009\n",
      "step 125000, loss 0.4574, acc 0.8958\n",
      "step 125500, loss 0.6965, acc 0.9015\n",
      "step 126000, loss 0.1064, acc 0.9013\n",
      "step 126500, loss 0.6285, acc 0.8980\n",
      "step 127000, loss 0.6145, acc 0.8999\n",
      "step 127500, loss 0.3580, acc 0.8969\n",
      "step 128000, loss 0.5031, acc 0.9006\n",
      "step 128500, loss 0.1165, acc 0.9002\n",
      "step 129000, loss 0.6347, acc 0.9015\n",
      "step 129500, loss 0.3404, acc 0.8975\n",
      "step 130000, loss 0.0862, acc 0.9001\n",
      "step 130500, loss 0.3002, acc 0.8971\n",
      "step 131000, loss 0.3027, acc 0.9025\n",
      "step 131500, loss 0.7984, acc 0.8996\n",
      "step 132000, loss 0.3415, acc 0.8984\n",
      "step 132500, loss 0.4554, acc 0.8981\n",
      "step 133000, loss 1.0729, acc 0.9008\n",
      "step 133500, loss 0.1139, acc 0.9010\n",
      "step 134000, loss 0.0489, acc 0.9002\n",
      "step 134500, loss 0.0572, acc 0.9001\n",
      "step 135000, loss 0.4221, acc 0.9022\n",
      "step 135500, loss 0.1500, acc 0.8973\n",
      "step 136000, loss 0.1897, acc 0.8993\n",
      "step 136500, loss 0.2779, acc 0.9029\n",
      "step 137000, loss 0.1547, acc 0.8974\n",
      "step 137500, loss 0.1802, acc 0.9018\n",
      "step 138000, loss 0.3529, acc 0.9010\n",
      "step 138500, loss 0.2382, acc 0.8995\n",
      "step 139000, loss 0.1201, acc 0.9018\n",
      "step 139500, loss 0.0481, acc 0.8998\n",
      "step 140000, loss 0.4167, acc 0.9012\n",
      "step 140500, loss 0.3774, acc 0.8995\n",
      "step 141000, loss 0.6201, acc 0.8984\n",
      "step 141500, loss 0.0639, acc 0.8999\n",
      "step 142000, loss 0.5962, acc 0.8990\n",
      "step 142500, loss 0.4058, acc 0.9009\n",
      "step 143000, loss 0.3731, acc 0.9020\n",
      "step 143500, loss 0.2523, acc 0.9016\n",
      "step 144000, loss 0.2547, acc 0.9003\n",
      "step 144500, loss 0.1258, acc 0.8997\n",
      "step 145000, loss 0.2257, acc 0.8984\n",
      "step 145500, loss 0.3720, acc 0.9031\n",
      "step 146000, loss 0.5649, acc 0.9009\n",
      "step 146500, loss 0.7577, acc 0.9039\n",
      "step 147000, loss 0.3624, acc 0.8982\n",
      "step 147500, loss 0.2634, acc 0.8997\n",
      "step 148000, loss 0.2242, acc 0.9028\n",
      "step 148500, loss 0.2992, acc 0.9029\n",
      "step 149000, loss 0.4293, acc 0.8981\n",
      "step 149500, loss 0.0971, acc 0.9030\n",
      "step 150000, loss 0.3368, acc 0.9038\n",
      "step 150500, loss 0.3766, acc 0.9005\n",
      "step 151000, loss 0.3424, acc 0.9009\n",
      "step 151500, loss 0.6331, acc 0.9010\n",
      "step 152000, loss 0.5240, acc 0.9023\n",
      "step 152500, loss 0.0635, acc 0.9023\n",
      "step 153000, loss 0.1297, acc 0.9015\n",
      "step 153500, loss 0.1800, acc 0.8978\n",
      "step 154000, loss 0.7637, acc 0.9044\n",
      "step 154500, loss 0.4084, acc 0.9040\n",
      "step 155000, loss 0.0580, acc 0.9017\n",
      "step 155500, loss 0.0822, acc 0.9010\n",
      "step 156000, loss 0.3911, acc 0.9008\n",
      "step 156500, loss 0.4157, acc 0.9011\n",
      "step 157000, loss 0.6795, acc 0.9032\n",
      "step 157500, loss 0.0581, acc 0.9010\n",
      "step 158000, loss 0.2389, acc 0.9032\n",
      "step 158500, loss 0.3586, acc 0.9004\n",
      "step 159000, loss 0.4562, acc 0.9046\n",
      "step 159500, loss 0.2202, acc 0.9019\n",
      "step 160000, loss 0.2253, acc 0.9031\n",
      "step 160500, loss 0.0885, acc 0.9021\n",
      "step 161000, loss 0.2781, acc 0.9059\n",
      "step 161500, loss 0.3724, acc 0.9019\n",
      "step 162000, loss 0.6902, acc 0.9032\n",
      "step 162500, loss 0.1748, acc 0.9037\n",
      "step 163000, loss 0.1137, acc 0.9037\n",
      "step 163500, loss 0.4332, acc 0.9016\n",
      "step 164000, loss 0.5265, acc 0.9038\n",
      "step 164500, loss 0.7038, acc 0.9009\n",
      "step 165000, loss 0.6608, acc 0.9043\n",
      "step 165500, loss 0.2850, acc 0.9012\n",
      "step 166000, loss 0.2399, acc 0.9076\n",
      "step 166500, loss 0.3575, acc 0.9013\n",
      "step 167000, loss 0.8681, acc 0.9022\n",
      "step 167500, loss 0.3686, acc 0.9021\n",
      "step 168000, loss 0.1915, acc 0.9066\n",
      "step 168500, loss 0.3726, acc 0.9014\n",
      "step 169000, loss 0.5680, acc 0.9022\n",
      "step 169500, loss 0.2018, acc 0.9045\n",
      "step 170000, loss 0.2700, acc 0.9054\n",
      "step 170500, loss 0.3645, acc 0.9047\n",
      "step 171000, loss 0.1514, acc 0.9040\n",
      "step 171500, loss 0.3997, acc 0.9026\n",
      "step 172000, loss 0.8658, acc 0.9040\n",
      "step 172500, loss 0.4188, acc 0.9054\n",
      "step 173000, loss 0.1811, acc 0.9055\n",
      "step 173500, loss 0.2451, acc 0.9047\n",
      "step 174000, loss 0.7438, acc 0.9024\n",
      "step 174500, loss 0.4966, acc 0.9066\n",
      "step 175000, loss 0.0263, acc 0.9031\n",
      "step 175500, loss 0.7048, acc 0.9053\n",
      "step 176000, loss 0.2867, acc 0.9035\n",
      "step 176500, loss 0.3159, acc 0.9058\n",
      "step 177000, loss 0.2141, acc 0.9021\n",
      "step 177500, loss 0.3268, acc 0.9061\n",
      "step 178000, loss 0.8509, acc 0.9069\n",
      "step 178500, loss 0.5096, acc 0.9021\n",
      "step 179000, loss 0.2595, acc 0.9060\n",
      "step 179500, loss 0.2160, acc 0.9049\n",
      "step 180000, loss 0.2730, acc 0.9041\n",
      "step 180500, loss 0.0610, acc 0.9025\n",
      "step 181000, loss 0.1592, acc 0.9044\n",
      "step 181500, loss 0.2282, acc 0.9041\n",
      "step 182000, loss 1.0730, acc 0.9065\n",
      "step 182500, loss 0.4239, acc 0.9059\n",
      "step 183000, loss 0.2932, acc 0.9034\n",
      "step 183500, loss 0.3921, acc 0.9037\n",
      "step 184000, loss 0.2365, acc 0.9056\n",
      "step 184500, loss 0.1104, acc 0.9048\n",
      "step 185000, loss 0.2576, acc 0.9038\n",
      "step 185500, loss 0.2563, acc 0.9064\n",
      "step 186000, loss 0.2939, acc 0.9066\n",
      "step 186500, loss 0.2199, acc 0.9051\n",
      "step 187000, loss 0.3569, acc 0.9049\n",
      "step 187500, loss 0.1539, acc 0.9028\n",
      "step 188000, loss 0.4520, acc 0.9066\n",
      "step 188500, loss 0.3256, acc 0.9071\n",
      "step 189000, loss 0.3696, acc 0.9029\n",
      "step 189500, loss 0.4984, acc 0.9035\n",
      "step 190000, loss 0.4884, acc 0.9067\n",
      "step 190500, loss 0.6779, acc 0.9078\n",
      "step 191000, loss 0.5563, acc 0.9071\n",
      "step 191500, loss 0.2511, acc 0.9014\n",
      "step 192000, loss 0.3108, acc 0.9089\n",
      "step 192500, loss 0.1562, acc 0.9014\n",
      "step 193000, loss 0.0561, acc 0.9062\n",
      "step 193500, loss 0.2800, acc 0.9058\n",
      "step 194000, loss 0.4482, acc 0.9047\n",
      "step 194500, loss 0.2576, acc 0.9077\n",
      "step 195000, loss 0.3661, acc 0.9042\n",
      "step 195500, loss 0.4619, acc 0.9059\n",
      "step 196000, loss 0.8262, acc 0.9070\n",
      "step 196500, loss 0.3359, acc 0.9054\n",
      "step 197000, loss 0.1725, acc 0.9042\n",
      "step 197500, loss 0.4884, acc 0.9070\n",
      "step 198000, loss 0.8721, acc 0.9047\n",
      "step 198500, loss 0.2236, acc 0.9085\n",
      "step 199000, loss 0.2623, acc 0.9040\n",
      "step 199500, loss 0.2788, acc 0.9073\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10000):  # 训练次数\n",
    "        batch_images, batch_labels = mnist.train.next_batch(32)\n",
    "        res_loss, _ = sess.run(\n",
    "            [loss, train_op], \n",
    "            feed_dict={\n",
    "            inputs: batch_images,\n",
    "            labels: batch_labels\n",
    "        })\n",
    "        if step % 500 == 0:\n",
    "            accs = []\n",
    "            for j in range(10000 // 32):\n",
    "                batch_images, batch_labels = mnist.test.next_batch(32)\n",
    "                res_acc = sess.run(\n",
    "                    acc,\n",
    "                    feed_dict={\n",
    "                        inputs: batch_images,\n",
    "                        labels: batch_labels\n",
    "                    })\n",
    "                accs.append(res_acc)\n",
    "\n",
    "            m_acc = np.mean(accs)\n",
    "            print('step %5d, loss %2.4f, acc %.4f' % (step, res_loss, m_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
